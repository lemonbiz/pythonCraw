该脚本程序是为了爬取“人民法院诉讼资产网”，网址为“http://www.rmfysszc.gov.cn/”，爬取各个省份的公告信息，解析出
公告中拍卖资产的名称
、地址、建筑性质、来源网站、省、市、区县、公告日期、起拍价、保证金、最小加价幅度、交易地点、公告编号、建筑面积、土地面积、评估价等信息。

本脚本用到的库如下：
1. import requests 用该库中的post函数向服务器发包，来抓取数据
2. import re 用该库来正则匹配
3. import urllib.request as urllib2 该脚本本来用Python2进行开发，后改成Python3，为了方便，把Python3中的urllib.request库改名为Python2的urllib2，
4. import urllib.parse as urlparse 同上，为了方便，把Python3中的urllib.parse改名为Python2的urlparse
5. import random  随机数的产生
6. from datetime import datetime, timedelta 日期库，对日期进行操作
7. import time 时间库
8. import lxml.html  解析HTML页面库
9. import socket 进程通信库，用来在下载页面的时候保持通信，防止过快爬取页面而被查封IP
10. import os  本地文件操作库
11. import threading  线程操作库，本脚本是用多线程爬取
12. import sys  系统库
13. import json  json文件操作库 
14. import calendar  日期操作库
15. import traceback  异常处理库
16. import copy  复制内存库
17. from pyquery import PyQuery as pq  解析文本的库
18. from lxml import etree  解析文本的库
19. import pymysql  操作数据库的库

本脚本的模块如下：
1. main					main()函数为程序入口地址，调用
						thread_post_prce_noce模块中的threaded_crawler()函数

2. thread_post_noce 	threaded_crawler()函数创建多个线程爬取各个省份的公告列表，调用
						io_log模块中的read_log()函数读取各个省份的日志文件，根据日志文件调用
						crwl_prce_inon模块中的downloader()函数爬取具体省份的公告列表

3. craw_prce_inon 		downloader()下载具体省份的公告列表，调用
						from rese_get_reou模块中的get_response()函数并根据日志文件中的信息向服务器发包然后获取数据，调用
						from rese_get_reou模块中的resolution解析数据，拿到公告的链接、发布日期、法院名称、公告标题，爬取一段时间后，调用
						from io_result模块的write_result()函数下载具体公告信息调用
						add_three_months和add_one_day改变从日志中读取来的数据调用
						io_log模块中的write_log()函数写日志

4. io_result 			write_result()函数，调用
						down_noce_inon模块中的hreaded_download()函数爬取公告具体内容，并调用回调函数
						html_exct模块中的extract()解析文本

5. down_noce_inon 		hreaded_download()函数根据url列表下载，并调用回调函数
						html_exct库中的extract()函数解析文本内容和对解析出的数据入库

6. html_exct			extract()函数调用
						change_to_price()函数和
						change_to_area()函数解析文本，然后调用
						write_to_db()函数对数据入库

7. io_log				write_log()把当前下载情况写入日志文件，
						read_log()从各个省份的目录下读取日志文件，然后在读取出来的数据作为参数传递给downloader()，然后决定从什么地方继续爬取

8. rese_get_reou 		get_response()函数post(data)向服务器请求数据(data是从日志文件中读取出来的数据)，
						resolution()函数是将从get_response()函数中返回的数据解析，提取每条公告的url,title,declaration time,court name，然后组成元组放在列表中返回

9. down_class			该模块定义了DiskCache,Throttle,Downloader三个类，
						DiskCache是将一个页面根据链接地址映射到本地保存，避免下次爬取的时候重复下载，提高效率
						Throttle实现了对同一个域名的服务器多次请求时随机降低访问速率，防止被查封IP
						Downloader是下载页面的类，将DiskCache和Throttle两个类的实例作为属性来提高下载效率

函数调用关系如下:
--main()	
	--threaded_crawler()开启多线程，每个线程
		--read_log(province_id) --- return from_data
		--downloader(from_data, province_id)
			--get_response(from_data, province_id) --- return content
			--resolution(content, province_id) --- return url_information_list
			--write_result(from_data, url_information_list, province_id)
				--threaded_download(url_information_list, cache=DiskCache(), scrape_callback=extract)开启多线程，每个线程
					--D = Download(cache..)
					--process_down()
						--D(url) --- return html
						--extract(source_id, html, [notice_title, declaration_time, province_name]) 
							--get_search(re.search())
							--change_to_price(any_type_data) --- return price 单位:分 [type(price) == int]
							--change_to_area(any_type_data) --- return area [type(area) == str]
							--write_to_db(data)
			--add_one_day(date) --- return date + one day
			--add_three_months(date) --- return date + three months
			--write_log(from_data, provined_id)

实习第一天主管就让我接收这个项目，其实也不能算项目，就是一个爬虫脚本，对一个网站的数据进行爬取，我当时大概看了网站，觉得应该挺简单的，就开始了近三礼拜的爬虫搬砖工程
第一个错误就产生了，没有具体分析网站数据的加载形式和发包情况，就觉得每个省份的公告列表时动态加载出来的，不能根绝URL加参数page去获取每一页的公告连接地址，然后我也没有向主管和学长请教，就急急忙忙的开始写代码，这个很不好的习惯，拿到一个项目，首先应该分析具体实现方案，而不是直接写代码。
然后我就用Python2写代码，因为我参考的《用Python写网络爬虫》是用Python2实现的，一开始写了一个脚本来爬取各个省份的入口地址，后来发现是很蠢的行为，因为每个省份的入口地址是相似的，就省份的代码不同，我还花了半天的时间去爬取和保存这些入口链接，然后再去读取，后来完全没有必要，我把省份的代码和名称组成了一个字典，方便遍历和查询。
因为没有具体分析各个省份公告列表中数据的加载方式，所以我就以为一定要用模拟浏览器来实现公告链接的抓取，使用了selenium库，在使用selenium库的过程中，问题百出：
1.对浏览器驱动的安装，要安装在path路径下，或者放在跟脚本文件同一个目录下，Linux系统上要把chromedrive的权限改成777，然后也要放在usr路径下
2.在标签查找时，如果找到了这个标签并用变量记录他的位置，然后如果数据页面数据改变了，那么这个标签也就找不到了
3.用driver.find_element_by_id('time1').send_keys('**')改变输入的时间后，用driver.find_element_by_id('search_sub').click()模拟鼠标点击，然后继续加载页面，但同时代码已经在寻找公告列表中公告的标签了，由于页面变化太慢，代码执行太快，就会出现时间改变之后的第一页抓取到的内容还是时间改变之前最后一页的内容，我尝试过睡眠几秒，理论上是可以解决的，但是不能解决这个问题，所以我只能尽可能的增加时间的跨度，这样损失的数据就会少。
虽然爬取速度很慢，但可以在有界面的电脑上可以运行，我们需要在服务器上运行，我就尝试用无界面的PhantomJS来加载，安装了它的驱动，但还是出错，报错提示selenium不支持PhantomJS了。主管还问我能不能用其他的方式抓取，我很肯定的说没有了，只能动态加载，然后主管就让人重装了服务器。然后我就在服务器上安装环境，然后还是各种跑不通，当我一筹莫展的时候，有个学长看了看网站，他说应该可以
用post发包的方式来请求服务器，抓包可以看到产生了一个handler.aspx包，它
General:
Request URL:http://www1.rmfysszc.gov.cn/News/handler.aspx
Request Method: POST

Request Headers:
Accept: application/json, text/javascript, */*; q=0.01
Accept-Encoding: gzip, deflate
Accept-Language: zh-CN,zh;q=0.9
Connection: keep-alive
Content-Length: 58
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
Cookie: __jsluid=b53ab071b6f687313d0c67a7cfba80e0; UM_distinctid=162f04fcde8ae-0f54db6b123c1c-3b720b58-100200-162f04fcde91a2; ASP.NET_SessionId=uowmrgjims4z30hvohumhc11; rmfysszc.gov.cn=20111241; CNZZDATA3765988=cnzz_eid%3D411153889-1524449140-http%253A%252F%252Fwww.rmfysszc.gov.cn%252F%26ntime%3D1524789514
Host: www1.rmfysszc.gov.cn
Origin: http://www1.rmfysszc.gov.cn
Referer: http://www1.rmfysszc.gov.cn/News/Pmgg.shtml?fid=102&dh=3&st=0
User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.117 Safari/537.36
X-Requested-With: XMLHttpRequest

From Data:
search: 
fid1: 102
fid2: 
fid3: 
time: 
time1: 
page: 2
include: 0

我将Request headers和From Data组成字典加在
requests.post(URL, data=from_data, headers=request_headers, timeout=30)
这样修改from_data中的数据就可以拿到相应的数据了
要把Request Headers中的Cookie去掉，因为cookie是辨别用户身份、进行 session 跟踪而储存在用户本地终端上的数据，为了在不同的电脑也能运行脚本，就不在headers中加cookie
日志内容就是根据from_data来写
这就是我的第一个错误


第二个错误就是我的代码耦合度太高了，我时函数内部调用函数，是面向过程的，现在想想，耦合度应该也不是很高，因为我把每一个功能都单独写成了一个模块，，不足的是我在html_exct模块中的extract()函数中，我没有把解析不同字段的代码提出来单独写成一个函数，导致这个函数很长，很难维护，我应该把每个字段的解析写成一个单独的函数，然后在extract()函数中调用它们，这样代码就会整洁。


第三个错误就是我代码中条件、循环的嵌套太多，有时候有好几重嵌套，这样也是不好的

主管建议我用分布式部署爬虫，然而我太懒一直没有学







